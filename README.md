# ğŸš€ IBM MQ MCP Ecosystem

A powerful and user-friendly **Model Context Protocol (MCP)** server for IBM MQ, providing seamless natural language interaction with your Queue Managers. Manage, monitor, and troubleshoot IBM MQ infrastructure through AI-powered assistants or simple web interfaces.

---

## âœ¨ Core Features

*   ğŸ” **Infrastructure Discovery**: Instantly list all queue managers and their statuses.
*   ğŸ“Š **Deep Monitoring**: Check queue depths, message status, and channel health with natural language.
*   ğŸ§  **Smart Workflows**: Automatically locates queues across any Queue Manager (including Clusters) without needing explicit targeting.
*   ğŸ›¡ï¸ **Installation Auditing**: Retrieve detailed MQ version, build, and installation path info.
*   ğŸ”’ **Production Protection**: Hostname-based filtering prevents accidental queries to production systems.
*   ğŸ¯ **Intelligent Query Routing**: Smart queue listing validates hostnames before executing MQ commands.
*   ğŸ” **Tool Transparency**: Configurable logging shows which MCP tools are called and their REST API endpoints.
*   ğŸ¤– **Multiple Interfaces**: Choose between Pattern-based (Basic), AI-powered (OpenAI / Anthropic / Gemini), Guided (One-click), or SSE (Real-time).
*   ğŸŒ **Universal REST Support**: Fully integrated with the IBM MQ REST API (mqweb), supporting both Distributed and z/OS managers.

---

## ğŸ› ï¸ The MQ Toolset

The server exposes three primary tools to any connected MCP client:

1.  **`dspmq`**: Lists available queue managers and their current state (Running/Ended).
2.  **`dspmqver`**: Provides detailed IBM MQ version, build level, and installation platform details.
3.  **`runmqsc`**: The powerhouse toolâ€”executes any MQSC command (e.g., `DISPLAY QLOCAL`, `DISPLAY CHSTATUS`) and returns formatted results.
4.  **`search_qmgr_dump`**: Instantly searches a local snapshot (`qmgr_dump.csv`) for any string (Queue, Channel, App ID) across all Queue Managers.

---

## ğŸ“‚ Project Architecture

```text
mqmcp/
â”œâ”€â”€ server/
â”‚   â””â”€â”€ mqmcpserver.py              # âš¡ FastMCP Server (The Core)
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ qmgr_dump.csv               # ğŸ“„ Offline Snapshot Data
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ providers/                  # ğŸ§© LLM Provider Modules
â”‚   â”‚   â”œâ”€â”€ __init__.py             #    Registry â€” get_provider("openai"|"anthropic"|"gemini")
â”‚   â”‚   â”œâ”€â”€ base.py                 #    Abstract LLMProvider base class
â”‚   â”‚   â”œâ”€â”€ openai_provider.py      #    OpenAI GPT handler
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py   #    Anthropic Claude handler
â”‚   â”‚   â””â”€â”€ gemini_provider.py      #    Google Gemini handler
â”‚   â”œâ”€â”€ mq_tools/                   # ğŸ“š Shared MQ Knowledge
â”‚   â”‚   â”œâ”€â”€ prompts.py              #    MQ_SYSTEM_PROMPT (single source of truth)
â”‚   â”‚   â”œâ”€â”€ schemas.py              #    Tool schemas for all providers (built from one list)
â”‚   â”‚   â””â”€â”€ converters.py           #    Dynamic MCP â†’ provider schema converters
â”‚   â”œâ”€â”€ streamlit_guided_client.py  # ğŸ§­ Guided Assistant (Recommended)
â”‚   â”œâ”€â”€ streamlit_basic_client.py   # ğŸ¤– Pattern-based Web UI
â”‚   â”œâ”€â”€ streamlit_openai_client.py  # ğŸ§  AI Assistant (LLM-powered)
â”‚   â”œâ”€â”€ streamlit_sse_client.py     # âš¡ SSE Client (Real-time)
â”‚   â”œâ”€â”€ streamlit_remote_client.py  # ğŸŒ Remote SSE + AI Client
â”‚   â”œâ”€â”€ llm_client.py               # ğŸ”— LLM Orchestrator (stdio mode)
â”‚   â”œâ”€â”€ dynamic_client.py           # ğŸ“œ Pattern Detection Library
â”‚   â””â”€â”€ test_mcp_client.py          # ğŸ§ª Developer CLI Menu
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_mq_api.bat / .sh        # ğŸ Server Launcher
â”‚   â””â”€â”€ run_*.bat / .sh             # ğŸš€ Client Launchers
â”œâ”€â”€ run_all_assistants.bat          # ğŸš€ Unified Launch Script
â”œâ”€â”€ .env                            # ğŸ” Secrets & Configuration
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Core Dependencies
â””â”€â”€ requirements-llm.txt            # ğŸ“¦ Optional LLM Dependencies
```

---

## ğŸš€ Quick Start

### 0. MQ REST API Prerequisite
Before running the ecosystem, ensure the IBM MQ REST API (mqweb) is active on your MQ server.
- **Check Status**: Run `dspmqweb status` on the MQ server.
- **Start Service**: Run `strmqweb` if it is not already running.

### 1. Environment Setup

We recommend using a Python virtual environment:

```powershell
# Create & Activate
python -m venv venv
.\venv\Scripts\Activate.ps1   # Windows
source venv/bin/activate      # Linux/Mac

# Install core dependencies
pip install -r requirements.txt

# Install LLM dependencies (OpenAI / Anthropic / Gemini â€” pick what you need)
pip install -r requirements-llm.txt
```

### 2. Configuration (`.env`)

Create a `.env` file in the root directory (or update the existing one):

```env
MQ_URL_BASE=https://your-host:9443/ibmmq/rest/v3/admin/
MQ_USER_NAME=mqreader
MQ_PASSWORD=your_password

# Production Protection: Allowed hostname prefixes (comma-separated)
MQ_ALLOWED_HOSTNAME_PREFIXES=lod,loq,lot

# Tool Logging Display (true/false)
MQ_SHOW_TOOL_LOGGING=true

# Server Logging Level (INFO, DEBUG, WARNING, ERROR)
MQ_LOG_LEVEL=INFO

# LLM Provider API Keys (add whichever providers you plan to use)
OPENAI_API_KEY=sk-...          # OpenAI GPT-4
ANTHROPIC_API_KEY=sk-ant-...   # Anthropic Claude
GEMINI_API_KEY=AIza...         # Google Gemini

# Server Configuration (SSE mode only)
MQ_MCP_HOST=0.0.0.0
MQ_MCP_PORT=5000
MQ_MCP_TRANSPORT=stdio   # 'stdio' (default) or 'sse'
```

### 3. Launching the Assistant

Choose your preferred flavor of the assistant:

| Assistant | Command | Best For... |
| :--- | :--- | :--- |
| **Unified Launch** | `.\run_all_assistants.bat` | **Launches ALL clients simultaneously.** |
| **Guided Assistant** | `streamlit run clients/streamlit_guided_client.py` | One-click ops & guided troubleshooting. |
| **AI Assistant (OpenAI)** | `streamlit run clients/streamlit_openai_client.py` | Natural conversations, Cluster support. |
| **Remote AI Assistant** | `streamlit run clients/streamlit_remote_client.py` | OpenAI / Anthropic / Gemini via SSE endpoint. |
| **SSE Assistant** | `streamlit run clients/streamlit_sse_client.py` | Real-time events & Smart Workflows. |
| **Basic Assistant** | `streamlit run clients/streamlit_basic_client.py` | Fast, deterministic pattern matching. |
| **CLI Tester** | `python clients/test_mcp_client.py` | Developers testing tool responses. |

---

## ğŸ§ª Developer Testing

### 1. Built-in CLI Tester
The project includes a dedicated tester script for quick validation:
```powershell
python clients/test_mcp_client.py
```

### 2. Standalone MCP Inspector
Use the official MCP Inspector to interact with the server via a web interface. This is great for debugging tool outputs.
```powershell
# Ensure venv is active
npx @modelcontextprotocol/inspector python server/mqmcpserver.py
```

### 3. Claude Desktop Integration
To use this server with Claude Desktop, add it to your configuration:
- **Path**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "ibm-mq": {
      "command": "python",
      "args": ["C:/absolute/path/to/mqmcp/server/mqmcpserver.py"],
      "env": {
        "MQ_URL_BASE": "https://host:port/ibmmq/rest/v3/admin/",
        "MQ_USER_NAME": "mq-user",
        "MQ_PASSWORD": "mq-password"
      }
    }
  }
}
```

### 4. VS Code Extensions (Cline / Roo Code)
Add the server to your extension settings using the same `command`, `args`, and `env` parameters as above. This allows you to manage MQ directly from your IDE's AI assistant.

---

## ğŸ›¡ï¸ Security & Stability

*   **Process Isolation**: Clients launch the MCP server as a dedicated subprocess utilizing the active virtual environment.
*   **Error Handling**: Built-in protection against Windows encoding issues (`UnicodeEncodeError`) and robust `.env` discovery.
*   **Logging**: Server-side diagnostic messages use Python's `logging` module and are routed to `stderr` to keep `stdout` clean for the MCP JSON-RPC protocol.
*   **Provider Modularity**: Each LLM provider lives in its own module under `clients/providers/`. Adding a new provider requires only one new file and one registry entry.

